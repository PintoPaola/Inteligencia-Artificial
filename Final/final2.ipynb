{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWIMl9dL+KEuZyd7W9vdWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PintoPaola/Inteligencia-Artificial/blob/main/Final/final2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class Board:\n",
        "    def __init__(self, rows=3, cols=3):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.state = self.generate_puzzle()  # Inicializa el estado del tablero aleatorio\n",
        "        self.goal_state = self.generate_goal()  # Inicializa el estado objetivo del tablero\n",
        "\n",
        "    def generate_puzzle(self):\n",
        "        state = np.arange(self.rows * self.cols)  # Crea una secuencia del tamaño del tablero\n",
        "        np.random.shuffle(state)  # Mezcla aleatoriamente la secuencia\n",
        "        return state.reshape((self.rows, self.cols))  # Devuelve el estado como una matriz reshaped\n",
        "\n",
        "    def generate_goal(self):\n",
        "        return np.append(np.arange(1, self.rows * self.cols), 0).reshape((self.rows, self.cols))\n",
        "        # Devuelve el estado objetivo del tablero, que es una secuencia ordenada\n",
        "\n",
        "    def valid_moves(self):\n",
        "        moves = []\n",
        "        zero_pos = np.argwhere(self.state == 0)[0]  # Posición del espacio vacío (0)\n",
        "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Direcciones posibles: arriba, abajo, izquierda, derecha\n",
        "        for d in directions:\n",
        "            new_pos = zero_pos + d\n",
        "            if 0 <= new_pos[0] < self.rows and 0 <= new_pos[1] < self.cols:\n",
        "                moves.append((zero_pos[0], zero_pos[1], new_pos[0], new_pos[1]))\n",
        "        return moves\n",
        "        # Devuelve una lista de movimientos válidos desde la posición del espacio vacío\n",
        "\n",
        "    def update(self, row1, col1, row2, col2):\n",
        "        self.state[row1, col1], self.state[row2, col2] = self.state[row2, col2], self.state[row1, col1]\n",
        "        # Actualiza el tablero intercambiando dos posiciones dadas\n",
        "\n",
        "    def is_game_over(self):\n",
        "        return np.array_equal(self.state, self.goal_state)\n",
        "        # Verifica si el estado actual del tablero es igual al estado objetivo\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.generate_puzzle()\n",
        "        # Resetea el tablero generando un nuevo estado aleatorio\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, alpha=0.5, prob_exp=0.5):\n",
        "        self.q_table = {}  # Tabla Q: estado -> valor de acción\n",
        "        self.alpha = alpha  # Tasa de aprendizaje\n",
        "        self.prob_exp = prob_exp  # Probabilidad de exploración\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "        # Método de reinicio del agente, en este caso no realiza ninguna operación\n",
        "\n",
        "    def move(self, board):\n",
        "        valid_moves = board.valid_moves()  # Obtiene los movimientos válidos del tablero\n",
        "        if not valid_moves:\n",
        "            return None  # Si no hay movimientos válidos, devuelve None\n",
        "\n",
        "        if np.random.uniform(0, 1) < self.prob_exp:\n",
        "            # Exploración: elige una acción aleatoria entre los movimientos válidos\n",
        "            return valid_moves[np.random.choice(len(valid_moves))]\n",
        "        else:\n",
        "            # Explotación: elige la acción con el mayor valor de acción según la tabla Q\n",
        "            max_value = -np.inf\n",
        "            best_move = None\n",
        "            for move in valid_moves:\n",
        "                state_key = tuple(board.state.flatten())  # Convierte el estado del tablero en una tupla hashable\n",
        "                if state_key not in self.q_table:\n",
        "                    self.q_table[state_key] = {}  # Crea una entrada para el estado en la tabla Q si no existe\n",
        "                if move not in self.q_table[state_key]:\n",
        "                    self.q_table[state_key][move] = 0  # Inicializa el valor de acción si no está definido\n",
        "\n",
        "                if self.q_table[state_key][move] >= max_value:\n",
        "                    max_value = self.q_table[state_key][move]  # Actualiza el máximo valor encontrado\n",
        "                    best_move = move  # Guarda la mejor acción encontrada\n",
        "\n",
        "            return best_move  # Devuelve la mejor acción encontrada\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = Agent(prob_exp=0.2)  # Ejemplo: probabilidad de exploración del 20%\n",
        "    board = Board()  # Crea un tablero\n",
        "\n",
        "    rounds = 1000  # Aumenta el número de rondas de entrenamiento\n",
        "\n",
        "    for _ in range(rounds):\n",
        "        move = agent.move(board)  # El agente decide el siguiente movimiento\n",
        "        if move is None:\n",
        "            board.reset()  # Resetea el tablero si no hay movimientos válidos (opcional)\n",
        "            continue  # Continúa con la siguiente ronda\n",
        "\n",
        "        board.update(move[0], move[1], move[2], move[3])  # Actualiza el tablero con el movimiento elegido\n",
        "\n",
        "    # Generación de la tabla Q\n",
        "    q_table_entries = []\n",
        "    for state_key, action_values in agent.q_table.items():\n",
        "        for action, value in action_values.items():\n",
        "            q_table_entries.append({'estado': state_key, 'acción': action, 'valor': value})\n",
        "\n",
        "    tabla_q = pd.DataFrame(q_table_entries)  # Crea un DataFrame de Pandas con los datos de la tabla Q\n",
        "    print(\"\\nTabla Q :\")\n",
        "    print(tabla_q)  # Imprime la tabla Q completa después de más rondas de entrenamiento\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VUJNRvrfhTs",
        "outputId": "fe068ba1-3748-4427-8792-d0f50b5774a2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tabla Q :\n",
            "                          estado        acción  valor\n",
            "0    (3, 6, 5, 4, 1, 8, 2, 0, 7)  (2, 1, 1, 1)      0\n",
            "1    (3, 6, 5, 4, 1, 8, 2, 0, 7)  (2, 1, 2, 0)      0\n",
            "2    (3, 6, 5, 4, 1, 8, 2, 0, 7)  (2, 1, 2, 2)      0\n",
            "3    (3, 6, 5, 4, 1, 8, 2, 7, 0)  (2, 2, 1, 2)      0\n",
            "4    (3, 6, 5, 4, 1, 8, 2, 7, 0)  (2, 2, 2, 1)      0\n",
            "..                           ...           ...    ...\n",
            "302  (2, 4, 8, 3, 0, 7, 1, 6, 5)  (1, 1, 1, 0)      0\n",
            "303  (2, 4, 8, 3, 0, 7, 1, 6, 5)  (1, 1, 1, 2)      0\n",
            "304  (2, 4, 8, 3, 7, 0, 1, 6, 5)  (1, 2, 0, 2)      0\n",
            "305  (2, 4, 8, 3, 7, 0, 1, 6, 5)  (1, 2, 2, 2)      0\n",
            "306  (2, 4, 8, 3, 7, 0, 1, 6, 5)  (1, 2, 1, 1)      0\n",
            "\n",
            "[307 rows x 3 columns]\n"
          ]
        }
      ]
    }
  ]
}